LINE-BY-LINE EXPLANATION: src/core/model.py
Model Architecture for Plant Disease Classification

================================================================================
PURPOSE OF THIS FILE:
================================================================================
This file defines the neural network architecture used for classifying plant
diseases. It uses a technique called "transfer learning" with ResNet50.

Think of it like this:
- ResNet50 = pre-trained expert that already knows how to recognize patterns
- We take this expert and teach it our specific task (plant diseases)
- Much faster and better than training from scratch

================================================================================
IMPORTS SECTION (Lines 15-19)
================================================================================

Line 15: import torch
- Main PyTorch library
- Provides tensor operations (like NumPy but for deep learning)

Line 16: import torch.nn as nn
- Neural network module
- Provides building blocks: layers, activations, loss functions

Line 17: from torchvision import models
- Pre-trained vision models
- We'll use models.resnet50 (50-layer deep network)

Line 18: from typing import Optional, Dict
- Type hints for better code documentation
- Optional[int] = can be int or None
- Dict = dictionary type

Line 19: from pathlib import Path
- Modern way to handle file paths

================================================================================
DISEASECLASS

IFIER CLASS DEFINITION (Lines 22-120)
================================================================================

Line 22: class DiseaseClassifier(nn.Module):
- Our main model class
- Inherits from nn.Module (base class for all PyTorch models)
- Must implement __init__ (setup) and forward (computation)

DOCSTRING (Lines 23-37):
- Multi-line documentation string
- Explains what the class does
- Lists architecture components
- Describes attributes

__init__ METHOD (Lines 39-106):
This is the constructor - runs when you create a new model instance.

Line 39-48: Method signature with parameters
- num_classes: How many disease types to classify (e.g., 38)
- pretrained: Whether to use pre-trained ImageNet weights (default: True)
- freeze_backbone: Whether to freeze ResNet layers (default: True)
- hidden_units: Size of hidden layer (default: 512)
- dropout_rate: Dropout probability (default: 0.5)

Line 61: super(DiseaseClassifier, self).__init__()
- Calls parent class (nn.Module) constructor
- Required for all PyTorch models
- Sets up internal PyTorch machinery

Lines 64-66: Store configuration as instance variables
- self.num_classes = num_classes
- These are saved so we can access them later (e.g., in save/load)

Lines 68-73: Load ResNet50 backbone
- if pretrained: Load weights trained on ImageNet
  * ImageNet = 1.4M images, 1000 classes
  * Weights = all the learned patterns (edges, textures, shapes)
- else: Start with random weights (training from scratch)
- ResNet50_Weights.DEFAULT = modern PyTorch way (replaces pretrained=True)

Lines 75-79: Freeze backbone if requested
- for param in self.backbone.parameters():
  * Loop through all weights in ResNet50
- param.requires_grad = False
  * Don't compute gradients (don't update during training)
- Why? Save computation, prevent overfitting
- We only want to train the final classification layers

Line 82-83: Get input feature size
- ResNet50's final layer expects 2048 input features
- num_features = self.backbone.fc.in_features
- fc = "fully connected" layer (the classifier)

Lines 85-106: Replace ResNet's classifier with custom head
- self.backbone.fc = nn.Sequential(...)
- Sequential = container that chains layers together
- Data flows: input → layer1 → layer2 → layer3 → output

  Layer 1 (Line 88): nn.Linear(num_features, hidden_units)
  - Linear = fully connected layer
  - Takes 2048 features, outputs 512 features
  - Compresses high-dimensional features to smaller representation
  - Formula: output = input @ weights + bias
  
  Layer 2 (Line 91-92): nn.ReLU(inplace=True)
  - ReLU = Rectified Linear Unit
  - Activation function: ReLU(x) = max(0, x)
  - Introduces non-linearity (allows learning complex patterns)
  - inplace=True = modify input directly (saves memory)
  
  Layer 3 (Line 95-96): nn.Dropout(p=dropout_rate)
  - Randomly zeros out 50% of neurons during training
  - Prevents overfitting (model memorizing training data)
  - During inference (prediction), all neurons are active
  - Think of it as "ensemble learning" - train multiple sub-networks
  
  Layer 4 (Line 99): nn.Linear(hidden_units, num_classes)
  - Final output layer
  - Takes 512 features, outputs num_classes values
  - Each output = "score" for that class
  - Higher score = more confident that disease is present

forward METHOD (Lines 108-131):
This defines the forward pass - how data flows through the network.

Line 108: def forward(self, x: torch.Tensor) -> torch.Tensor:
- x = input tensor (batch of images)
- Returns output tensor (predictions for each image)

Lines 111-116: Input shape documentation
- (batch_size, 3, 224, 224)
  * batch_size = how many images at once (e.g., 32)
  * 3 = RGB channels (Red, Green, Blue)
  * 224x224 = image width and height in pixels

Lines 118-120: Output shape documentation
- (batch_size, num_classes)
  * batch_size = same as input
  * num_classes = one score per disease type
- Raw logits = unnormalized scores
- To get probabilities: apply softmax

Line 130: return self.backbone(x)
- Pass input through the backbone (ResNet50 + our classifier)
- self.backbone(x) calls ResNet50's forward method
- Returns prediction logits

Why so simple? Because we replaced ResNet's fc layer with our custom one!
All the computation is inside self.backbone.

================================================================================
UTILITY METHODS (Lines 133-186)
================================================================================

get_trainable_parameters() (Lines 133-147):
- Counts how many parameters will be updated during training
- p.numel() = number of elements in parameter tensor
- p.requires_grad = whether parameter needs gradients
- sum(...) = add up all trainable parameters
- Used to show what percentage of model is being trained

get_total_parameters() (Lines 149-162):
- Counts ALL parameters (trainable + frozen)
- Same logic but without requires_grad filter
- Used to show model size

print_model_summary() (Lines 164-186):
- Pretty-prints model information
- Shows:
  * Total parameters (~25M for ResNet50)
  * Trainable parameters (~1M if backbone frozen)
  * Frozen parameters (~24M if backbone frozen)
  * Percentage trainable (~4% if backbone frozen)
- Useful for verifying model setup before training

================================================================================
CREATE_MODEL FUNCTION (Lines 189-230)
================================================================================

Line 189: def create_model(...):
- Factory function = convenient way to create models
- Wraps DiseaseClassifier creation with common setup steps

Lines 191-199: Function parameters
- Same as DiseaseClassifier.__init__
- Plus: device = where to load model ("cuda" or "cpu")

Lines 216-223: Function body
1. Create model instance
2. Move to device: model.to(device)
   - Transfers all parameters to GPU or CPU
   - Must do this before training
3. Print summary for verification
4. Return initialized model

Why use this instead of DiseaseClassifier directly?
- Convenience: handles device transfer automatically
- Debugging: always prints summary
- Consistency: ensures proper initialization

================================================================================
SAVE_MODEL FUNCTION (Lines 233-272)
================================================================================

Purpose: Save trained model to disk for later use.

Lines 239-244: Function parameters
- model = the model to save
- save_path = where to save (e.g., "models/best_model.pth")
- epoch = optional, current training epoch
- optimizer_state = optional, to resume training
- metrics = optional, validation accuracy, loss, etc.

Lines 257-261: Create checkpoint dictionary
- "model_state_dict" = all weights and biases
  * state_dict() extracts all learnable parameters
  * This is the actual "intelligence" of the model
- "model_config" = settings needed to reconstruct model
  * num_classes, hidden_units, dropout_rate
  * Required because architecture must match weights

Lines 264-270: Add optional information
- if epoch is not None: save epoch number
  * Useful for resuming training: "continue from epoch 15"
- if optimizer_state: save optimizer state
  * Optimizer has its own state (momentum, learning rates)
  * Needed to resume training seamlessly
- if metrics: save performance metrics
  * Track best accuracy, loss over time

Line 273-274: Create directory and save
- save_path.parent.mkdir(parents=True, exist_ok=True)
  * Create directories if they don't exist
- torch.save(checkpoint, save_path)
  * Serializes dictionary to file
  * .pth = PyTorch model file extension

Why save as dictionary instead of just model.state_dict()?
- Flexibility: can save multiple things in one file
- Compatibility: can reconstruct model even if code changes
- Debugging: can inspect epoch, metrics without loading model

================================================================================
LOAD_MODEL FUNCTION (Lines 275-342)
================================================================================

Purpose: Load a previously saved model from disk.

Lines 277-283: Function parameters
- load_path = where the model file is
- device = where to load it (GPU or CPU)
- for_inference = if True, set to eval mode (no dropout, no training)

Lines 296-298: Check if file exists
- Prevents cryptic errors if file missing
- Raises clear error with file path

Line 301: Load checkpoint from disk
- torch.load(load_path, map_location=device)
  * Loads the dictionary we saved
  * map_location = load to correct device (especially for CPU/GPU differences)

Lines 304-306: Extract model configuration
- Need to know num_classes, hidden_units, etc. to rebuild model
- checkpoint.get("model_config", {}) = get config or empty dict

Lines 308-313: Handle old models without config
- Backwards compatibility
- Try to infer num_classes from weight tensor shape
- Use default values for other settings
- Print warning so user knows

Lines 316-323: Reconstruct model architecture
- Create DiseaseClassifier with same configuration
- pretrained=False because we're loading our own weights (not ImageNet)
- freeze_backbone=False so we can load all weights (even if they were frozen during training)

Line 326: Load trained weights
- model.load_state_dict(checkpoint["model_state_dict"])
- This is where the "knowledge" is transferred
- After this, model has all the learned patterns

Lines 329-330: Move to device
- Same as in create_model
- Ensures model is on correct device (GPU or CPU)

Lines 333-335: Set to evaluation mode if for inference
- model.eval()
  * Disables dropout (want deterministic predictions)
  * Puts batch normalization in inference mode
  * Essential for consistent predictions

Lines 340-345: Print helpful information
- Show where model was loaded from
- Display training epoch if saved
- Display metrics if saved
- Helps with debugging and model selection

================================================================================
TEST CODE (Lines 345-376)
================================================================================

Line 345: if __name__ == "__main__":
- This block only runs when file is executed directly
- Doesn't run when file is imported
- Used for testing and demonstration

Test 1 (Lines 351-354): Create model
- Verify model can be instantiated
- Check model summary prints correctly
- Use CPU for testing (no GPU required)

Test 2 (Lines 357-363): Forward pass
- Create dummy input: 4 images, 3 channels, 224x224
- torch.randn = random numbers from normal distribution
- Pass through model: output = model(dummy_input)
- Verify shapes are correct:
  * Input: (4, 3, 224, 224)
  * Output: (4, 38) if num_classes=38
- Check output range (should be roughly -10 to +10 for logits)

Test 3 (Lines 366-373): Save and load
- Save model to temporary file
- Load model back
- Verify no errors
- Delete test file: test_path.unlink()
- This tests the complete save/load cycle

Why these tests?
- Catch errors early (before training)
- Verify architecture works correctly
- Ensure save/load cycle doesn't corrupt model
- Provide usage examples

================================================================================
HOW TO USE THIS FILE
================================================================================

CREATING A MODEL:
```python
from src.core.model import create_model

# Create new model for training
model = create_model(
    num_classes=38,
    pretrained=True,
    freeze_backbone=True,
    device="cuda"
)
```

SAVING A MODEL:
```python
from src.core.model import save_model
from pathlib import Path

# After training
save_model(
    model,
    save_path=Path("models/best_model.pth"),
    epoch=10,
    metrics={"accuracy": 0.95, "loss": 0.15}
)
```

LOADING A MODEL:
```python
from src.core.model import load_model
from pathlib import Path

# For inference
model = load_model(
    load_path=Path("models/best_model.pth"),
    device="cuda",
    for_inference=True
)

# Make prediction
with torch.no_grad():
    prediction = model(image_tensor)
```

================================================================================
KEY CONCEPTS EXPLAINED
================================================================================

TRANSFER LEARNING:
- Instead of training from scratch (random weights), start with pre-trained model
- Pre-trained model already learned basic patterns (edges, textures, shapes)
- We only need to teach it plant-specific patterns
- Result: Faster training, better accuracy, needs less data

FREEZING LAYERS:
- freeze_backbone=True means ResNet50 weights don't change
- Only train the final classifier layers
- Why?
  * Faster training (fewer parameters to update)
  * Prevents overfitting (backbone already good)
  * Can "unfreeze" later for fine-tuning if needed

MODEL STATE DICT:
- Dictionary containing all learnable parameters
- Keys = parameter names ("backbone.layer1.conv1.weight", etc.)
- Values = parameter tensors (the actual numbers)
- This is what gets updated during training

LOGITS VS PROBABILITIES:
- Model outputs logits (raw scores): [-3.2, 1.5, 0.8, -1.0, ...]
- Apply softmax to get probabilities: [0.01, 0.45, 0.22, 0.32]
- Probabilities sum to 1.0
- Highest probability = predicted class

DEVICE (GPU VS CPU):
- Model and data must be on same device
- "cuda" = NVIDIA GPU (fast but limited memory)
- "cpu" = CPU (slow but unlimited memory)
- .to(device) moves model/tensor to device

================================================================================
COMMON ISSUES AND SOLUTIONS
================================================================================

ISSUE: "CUDA out of memory"
SOLUTION: Reduce batch_size in config.py

ISSUE: "Size mismatch when loading state dict"
SOLUTION: Model architecture changed, can't load old weights
          Either retrain or match architecture to saved model

ISSUE: "Model not learning (loss not decreasing)"
SOLUTION: Check learning rate, verify data is correct, try unfreezing backbone

ISSUE: "Model overfitting (high training acc, low validation acc)"
SOLUTION: Increase dropout_rate, add more data augmentation, collect more data

================================================================================
OPTIMIZATION TECHNIQUES USED
================================================================================

1. TRANSFER LEARNING: Start with pre-trained weights
2. FREEZING: Only train necessary layers
3. DROPOUT: Prevent overfitting
4. BATCH PROCESSING: Process multiple images at once (GPU efficient)
5. INPLACE OPERATIONS: Save memory (e.g., ReLU(inplace=True))
6. STATE DICT SAVING: Only save weights, not entire model object

================================================================================
SUMMARY
================================================================================

This file provides a clean, well-documented interface for:
1. Creating disease classification models
2. Saving trained models to disk
3. Loading saved models for inference or continued training

The model uses ResNet50 with transfer learning for efficient, accurate
plant disease classification. All common operations (create, save, load)
are wrapped in convenient functions with comprehensive error handling.
